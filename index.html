<!DOCTYPE HTML>
<!-- saved from url=(0041)https://people.eecs.berkeley.edu/~barron/ -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Elnur Gasanov</title>

  <meta name="author" content="Elnur Gasanov">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/jpg" href="images/kaust-logo.jpg">
</head>

<body style="background-color: whitesmoke">
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Elnur Gasanov</name>
                <p> I am a final-year PhD candidate in the Optimization and Machine Learning Lab, part of the <a href="https://www.kaust.edu.sa/en/research/generative-ai">Center of Excellence for Generative AI (GenAI)</a> at <a href="https://kaust.edu.sa">King Abdullah University of Science and Technology (KAUST)</a>. Advised by Professor <a href="http://richtarik.org/index.html">Peter Richt&aacuterik</a>, I plan to defend my dissertation in mid-2025. My research focuses on distributed machine learning, stochastic optimization, and randomized linear algebra. </p>
                
                <p> I earned my Master of Science degree in Computer Science from <a href="https://kaust.edu.sa">KAUST</a> and my Bachelor's degree in Applied Mathematics and Physics from <a href="https://mipt.ru/english/">Moscow Institute of Physics and Technology</a>. </p>
              <p style="text-align:center">
                <a href="mailto:elnur.gasanov@kaust.edu.sa">Email</a> &nbsp/&nbsp
                <a href="data/CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/elnurgasanov/"> LinkedIn </a> &nbsp/&nbsp
                <a href="https://github.com/gaseln"> GitHub </a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/elnurgasanov.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/elnurgasanov-circle.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
        <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <heading style="color: blue;">News & Publications</heading>
                <p><strong>Recent Highlights:</strong></p>
                <ul>
                    <li> Exploring postdoctoral and industry research opportunities.</li>
                    <li> Preparing to defend my PhD dissertation in mid-2025.</li>
                    <li>  Successfully completed a Research Scientist internship at <a href="https://www.aramco.com">Saudi Aramco</a> in August 2024.</li>
                </ul>

                <p><strong>Recent Publications in Top Conferences & Journal</strong></p>
                <ul>
                    <li> <a href="https://openreview.net/pdf?id=Ch7WqGcGmb">"Error Feedback Reloaded: From Quadratic to Arithmetic Mean of Smoothness Constants"</a>, accepted to <a href="https://iclr.cc/Conferences/2024">ICLR 2024</a>.</li>
                    <li> <a href="https://proceedings.mlr.press/v238/szlendak24a/szlendak24a.pdf">"Understanding Progressive Training Through the Framework of Randomized Coordinate Descent"</a>, accepted to <a href="http://aistats.org/aistats2024/">AISTATS 2024</a>.</li>
                    <li> <a href="https://openreview.net/pdf?id=Rb6VDOHebB">"Adaptive Compression for Communication-Efficient Distributed Training"</a>, accepted in <a href="https://jmlr.org/tmlr/index.html">Transactions on Machine Learning Research</a>.</li>
                    <li> <a href="https://proceedings.mlr.press/v162/richtarik22a/richtarik22a.pdf">"3PC: Three Point Compressors for Communication-Efficient Distributed Training and a Better Theory for Lazy Aggregation"</a>, accepted to <a href="https://icml.cc/Conferences/2022">ICML 2022</a>.</li>
                    <li> <a href="https://arxiv.org/abs/2111.11556">"FLIX: A Simple and Communication-Efficient Alternative to Local Methods in Federated Learning"</a>, accepted to <a href="http://aistats.org/aistats2022/">AISTATS 2022</a>.</li>
                </ul>
                
                <p><em>Additional papers are currently under peer review and in preparation.</em></p>

                <p><strong>Selected Talks & Presentations:</strong></p>
                <ul>
                    <li> <strong>Megadata Federated Learning (July 2023):</strong> Gave a talk on "FLIX: A Simple and Communication-Efficient Alternative to Local Methods in Federated Learning" (<a href="data/MegaData_certificate.pdf">certificate</a>).</li>
                    <li> <strong>NFFL 2021 (NeurIPS Workshop):</strong> Presented a poster on FLIX (<a href="data/fedmix_poster.pdf">poster</a>).</li>
                    <li> <strong>Optimization without Borders Conference (July 2021):</strong> Presented a poster on FLIX (<a href="data/fedmix_poster.pdf">poster</a>, <a href="images/photo_sirius.jpeg">photo</a>).</li>
                    <li> <strong>KAUST-Tsinghua-Industry Workshop (Nov 2019):</strong> Presented a poster on randomized optimization (<a href="data/sscd_poster.pdf">poster</a>).</li>
                    <li> <strong>Data Science Summer School (DS3, June 2019):</strong> Presented "A New Randomized Method for Solving Large Linear Systems".</li>
                </ul>
            </td>
        </tr>
    </tbody>
</table>


     <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <heading style="color: blue;">Publications</heading><br><br>
                <div>
                <a href="https://arxiv.org/abs/2412.13619"><papertitle>Speeding up Stochastic Proximal Optimization in the High Hessian Dissimilarity Setting</papertitle></a><br>
                Stochastic proximal point methods have recently garnered renewed attention within the optimization community, primarily due to their desirable theoretical properties. Notably, these methods exhibit a convergence rate that is independent of the Lipschitz smoothness constants of the loss function, a feature often missing in the loss functions of modern ML applications. In this paper, we revisit the analysis of the Loopless Stochastic Variance Reduced Proximal Point Method (L-SVRP). Building on existing work, we establish a theoretical improvement in the convergence rate in scenarios characterized by high Hessian dissimilarity among the functions. Our concise analysis, which does not require smoothness assumptions, demonstrates a significant improvement in communication complexity compared to standard stochastic gradient descent.<br>
                Elnur Gasanov, <a href="https://richtarik.org/index.html">Peter Richt&aacuterik</a><br>
                <a href="https://arxiv.org/abs/2412.13619">arXiv</a>
                </div><br>
                
                <div>
                <a href="https://openreview.net/pdf?id=Ch7WqGcGmb"><papertitle>Error Feedback Reloaded: From Quadratic to Arithmetic Mean of Smoothness Constants</papertitle></a><br>
                Error feedback (EF) is a highly popular and immensely effective mechanism for fixing convergence issues which arise in distributed training methods (such as distributed GD or SGD) when these are enhanced with greedy communication compression techniques such as Top-k. While EF was proposed almost a decade ago (Seide et al, 2014), and despite concentrated effort by the community to advance the theoretical understanding of this mechanism, there is still a lot to explore. In this work we study a modern form of error feedback called EF21 (Richtárik et al, 2021) which offers the currently best-known theoretical guarantees, under the weakest assumptions, and also works well in practice. In particular, while the theoretical communication complexity of EF21 depends on the quadratic mean of certain smoothness parameters, we improve this dependence to their arithmetic mean, which is always smaller, and can be substantially smaller, especially in heterogeneous data regimes. We take the reader on a journey of our discovery process. Starting with the idea of applying EF21 to an equivalent reformulation of the underlying problem which (unfortunately) requires (often impractical) machine cloning, we continue to the discovery of a new weighted version of EF21 which can (fortunately) be executed without any cloning, and finally circle back to an improved analysis of the original EF21 method. While this development applies to the simplest form of EF21, our approach naturally extends to more elaborate variants involving stochastic gradients and partial participation. Further, our technique improves the best-known theory of EF21 in the rare features regime (Richtárik et al, 2023). Finally, we validate our theoretical findings with suitable experiments.<br>
                <a href="https://richtarik.org/index.html">Peter Richt&aacuterik</a>, Elnur Gasanov, <a href="https://burlachenkok.github.io/home">Konstantin Burlachenko</a><br>
                <a href="https://openreview.net/pdf?id=Ch7WqGcGmb">ICLR 2024</a>
                </div><br>
                
                <div>
                <a href="https://arxiv.org/pdf/2305.15264.pdf"><papertitle>Error Feedback Shines when Features are Rare</papertitle></a><br>
                We provide the first proof that gradient descent <span
                class="math inline">(<em>G</em><em>D</em>)</span> with greedy
                sparsification <span
                class="math inline">(<em>T</em><em>o</em><em>p</em><em>K</em>)</span>
                and error feedback <span
                class="math inline">(<em>E</em><em>F</em>)</span> can obtain better
                communication complexity than vanilla <span
                class="math inline"><em>G</em><em>D</em></span> when solving the
                distributed optimization problem <span
                class="math inline">min <em>f</em>(<em>x</em>) = 1/<em>n</em>∑<em>f</em><sub><em>i</em></sub>(<em>x</em>)</span>,
                where <span class="math inline"><em>n</em></span> = # of clients, <span
                class="math inline"><em>d</em></span> = # of features, and <span
                class="math inline"><em>f</em><sub>1</sub>, …, <em>f</em><sub><em>n</em></sub></span>
                are smooth nonconvex functions. Despite intensive research since 2014
                when <span class="math inline"><em>E</em><em>F</em></span> was first
                proposed by Seide et al., this problem remained open until now. Perhaps
                surprisingly, we show that <span
                class="math inline"><em>E</em><em>F</em></span> shines in the regime
                when features are rare, i.e., when each feature is present in the data
                owned by a small number of clients only. To illustrate our main result,
                we show that in order to find a random vector <span
                class="math inline"><em>x̂</em></span> such that <span
                class="math inline">∥∇<em>f</em>(<em>x̂</em>)∥<sup>2</sup> ≤ <em>ε</em></span>
                in expectation, <span class="math inline"><em>G</em><em>D</em></span>
                with the <span
                class="math inline"><em>T</em><em>o</em><em>p</em>1</span> sparsifier
                and <span class="math inline"><em>E</em><em>F</em></span> requires <span
                class="math inline"><em>O</em>((<em>L</em>+<em>r</em><em>√</em><em>c</em>/<em>n</em>min (<em>c</em>/<em>n</em>max<sub><em>i</em></sub><em>L</em><sub><em>i</em></sub><sup>2</sup>,1/<em>n</em>∑<em>L</em><sub><em>i</em></sub><sup>2</sup>))1/<em>ε</em>)</span>
                bits to be communicated by each worker to the server only, where <span
                class="math inline"><em>L</em></span> is the smoothness constant of
                <span class="math inline"><em>f</em></span>, <span
                class="math inline"><em>L</em><sub><em>i</em></sub></span> is the
                smoothness constant of <span
                class="math inline"><em>f</em><sub><em>i</em></sub></span>, <span
                class="math inline"><em>c</em></span> is the maximal number of clients
                owning any feature (<span
                class="math inline">1 ≤ <em>c</em> ≤ <em>n</em></span>), and <span
                class="math inline"><em>r</em></span> is the maximal number of features
                owned by any client (<span
                class="math inline">1 ≤ <em>r</em> ≤ <em>d</em></span>). Clearly, the
                communication complexity improves as <span
                class="math inline"><em>c</em></span> decreases (i.e., as features
                become more rare), and can be much better than the <span
                class="math inline"><em>O</em>(<em>r</em><em>L/</em><em>ε</em>)</span>
                communication complexity of <span
                class="math inline"><em>G</em><em>D</em></span> in the same regime.<br>
                <a href="https://richtarik.org/index.html">Peter Richt&aacuterik</a>, Elnur Gasanov, <a href="https://burlachenkok.github.io/home">Konstantin Burlachenko</a><br>
                <a href="https://arxiv.org/pdf/2305.15264.pdf">arXiv</a>
                </div><br>
                
                <div>
                <a href="https://arxiv.org/abs/2306.03626"><papertitle>Understanding Progressive Training Through the Framework of Randomized Coordinate Descent</papertitle></a><br>
                We propose a Randomized Progressive Training algorithm (RPT) -- a stochastic proxy for the well-known Progressive Training method (PT) (Karras et al., 2017). Originally designed to train GANs (Goodfellow et al., 2014), PT was proposed as a heuristic, with no convergence analysis even for the simplest objective functions. On the contrary, to the best of our knowledge, RPT is the first PT-type algorithm with rigorous and sound theoretical guarantees for general smooth objective functions. We cast our method into the established framework of Randomized Coordinate Descent (RCD) (Nesterov, 2012; Richtárik & Takáč, 2014), for which (as a by-product of our investigations) we also propose a novel, simple and general convergence analysis encapsulating strongly-convex, convex and nonconvex objectives. We then use this framework to establish a convergence theory for RPT. Finally, we validate the effectiveness of our method through extensive computational experiments.<br>
                <a href="https://www.linkedin.com/in/rafalszlendak/?originalSubdomain=pl">Rafal Szlendak</a>, Elnur Gasanov, <a href="https://richtarik.org/index.html">Peter Richt&aacuterik</a><br>
                <a href="https://proceedings.mlr.press/v238/szlendak24a/szlendak24a.pdf">AISTATS 2024</a>
                </div><br>
                
                <div>
                <a href="https://arxiv.org/abs/2211.00188"><papertitle>Adaptive Compression for Communication-Efficient Distributed Training</papertitle></a><br>
                We propose Adaptive Compressed Gradient Descent (AdaCGD) - a novel optimization algorithm for communication-efficient training of supervised machine learning models with adaptive compression level. Our approach is inspired by the recently proposed three point compressor (3PC) framework of Richtarik et al. (2022), which includes error feedback (EF21), lazily aggregated gradient (LAG), and their combination as special cases, and offers the current state-of-the-art rates for these methods under weak assumptions. While the above mechanisms offer a fixed compression level, or adapt between two extremes only, our proposal is to perform a much finer adaptation. In particular, we allow the user to choose any number of arbitrarily chosen contractive compression mechanisms, such as Top-K sparsification with a user-defined selection of sparsification levels K, or quantization with a user-defined selection of quantization levels, or their combination. AdaCGD chooses the appropriate compressor and compression level adaptively during the optimization process. Besides i) proposing a theoretically-grounded multi-adaptive communication compression mechanism, we further ii) extend the 3PC framework to bidirectional compression, i.e., we allow the server to compress as well, and iii) provide sharp convergence bounds in the strongly convex, convex and nonconvex settings. The convex regime results are new even for several key special cases of our general mechanism, including 3PC and EF21. In all regimes, our rates are superior compared to all existing adaptive compression methods. <br>
                <a href="https://www.linkedin.com/in/maksim-makarenko-a75a4b201">Maksim Makarenko</a>, Elnur Gasanov, <a href="https://rustem-islamov.github.io">Rustem Islamov</a>, <a href="https://sadiev.netlify.app">Abdurakhmon Sadiev</a>, <a href="https://richtarik.org/index.html">Peter Richt&aacuterik</a><br>
                <a href="https://openreview.net/pdf?id=Rb6VDOHebB">TMLR</a>
                </div><br>
                
                <div>
                <a href="https://arxiv.org/pdf/2202.00998.pdf"><papertitle>3PC: Three Point Compressors for Communication-Efficient Distributed Training and a Better Theory for Lazy Aggregation </papertitle></a><br>
                We propose and study a new class of gradient communication mechanisms for communication-efficient training—three point compressors (3PC)—as well as efficient distributed nonconvex optimization algorithms that can take advantage of them. Unlike most established approaches, which rely on a static compressor choice (e.g., Top-𝐾), our class allows the compressors to evolve throughout the training process, with the aim of improving the theoretical communication complexity and practical efficiency of the underlying methods. We show that our general approach can recover the recently proposed state-of-the-art error feedback mechanism EF21 (Richtarik et al., 2021) and its theoretical properties as a special case, but also leads to a number of new efficient methods. Notably, our approach allows us to improve upon the state-of-the-art in the algorithmic and theoretical foundations of the lazy aggregation literature (Chen et al., 2018). As a by-product that may be of independent interest, we provide a new and fundamental link between the lazy aggregation and error feedback literature. A special feature of our work is that we do not require the compressors to be unbiased. <br>
                <a href="https://richtarik.org/index.html">Peter Richt&aacuterik</a>, <a href="https://cemse.kaust.edu.sa/people/person/igor-sokolov">Igor Sokolov</a>, <a href="https://ai.ethz.ch/people/ilyas-fatkhullin.html">Ilyas Fatkhullin</a>, Elnur Gasanov, <a href="https://zhizeli.github.io/">Zhize Li</a>, <a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a><br>
                <a href="https://proceedings.mlr.press/v162/richtarik22a/richtarik22a.pdf">ICML 2022</a>
                </div><br>

                <div>
                <a href="https://arxiv.org/abs/2111.11556"><papertitle>FLIX: A Simple and Communication-Efficient Alternative to Local Methods in Federated Learning</papertitle></a><br>
                Federated Learning (FL) is an increasingly popular machine learning paradigm in which multiple nodes try to collaboratively learn under privacy, communication and multiple heterogeneity constraints. A persistent problem in federated learn- ing is that it is not clear what the optimization objective should be: the standard average risk minimization of supervised learning is inadequate in handling several major constraints specific to federated learning, such as communication adaptivity and personalization control. We identify several key desiderata in frameworks for federated learning and introduce a new framework, FedMix, that takes into account the unique challenges brought by federated learning. FedMix has a standard finite-sum form, which enables practitioners to tap into the immense wealth of existing (potentially non-local) methods for distributed optimization. Through a smart initialization that does not require any communication, FedMix does not re- quire the use of local steps but is still provably capable of performing dissimilarity regularization on par with local methods. We give several algorithms for solving the FedMix formulation efficiently under communication constraints. Finally, we corroborate our theoretical results with extensive experimentation.<br>
                Elnur Gasanov, <a href="https://rka97.github.io">Ahmed Khaled</a>, <a href="https://samuelhorvath.github.io">Samuel Horv&aacuteth</a>, <a href="https://richtarik.org/index.html">Peter Richt&aacuterik</a><br>
                <a href="https://proceedings.mlr.press/v151/gasanov22a/gasanov22a.pdf"> AISTATS 2022</a>
                </div><br>

                <div>
                <a href="https://fl-icml.github.io/2021/papers/FL-ICML21_paper_46.pdf"> <papertitle>Lower Bounds and Optimal Algorithms for Smooth and Strongly Convex Decentralized Optimization over Time-Varying Networks</papertitle></a><br>
                We consider the task of minimizing the sum of smooth and strongly convex functions stored in a decentralized manner across the nodes of a communication network whose links are allowed to change in time. We solve two fundamental problems for this task. First, we establish the first lower bounds on the number of decentralized communication rounds and the number of local computations required to find an ε-accurate solution. Second, we design two optimal algorithms that attain these lower bounds: (i) a variant of the recently proposed algorithm ADOM (Kovalev et al., 2021) enhanced via a multi-consensus subroutine, which is optimal in the case when access to the dual gradients is assumed, and (ii) a novel algorithm, called ADOM+, which is optimal in the case when access to the primal gradients is assumed. We corroborate the theoretical efficiency of these algorithms by performing an experimental comparison with existing state-of-the-art methods.<br>
                <a href="https://dmitry-kovalev.com">Dmitry Kovalev</a>, Elnur Gasanov, <a href="https://richtarik.org/index.html">Peter Richt&aacuterik</a>, <a href="https://www.hse.ru/en/org/persons/49503846"> Alexander Gasnikov</a><br>
                <a href="https://openreview.net/pdf?id=L8-54wkift">NeurIPS 2021</a>
                </div><br>

                <div>
                <a href="https://arxiv.org/abs/2004.01442"> <papertitle>From Local SGD to Local Fixed-Point Methods for Federated Learning</papertitle></a><br>
                Most algorithms for solving optimization problems or finding saddle points of convex-concave functions are fixed-point algorithms. In this work we consider the generic problem of finding a fixed point of an average of operators, or an approximation thereof, in a distributed setting. Our work is motivated by the needs of federated learning. In this context, each local operator models the computations done locally on a mobile device. We investigate two strategies to achieve such a consensus: one based on a fixed number of local steps, and the other based on randomized computations. In both cases, the goal is to limit communication of the locally-computed variables, which is often the bottleneck in distributed frameworks. We perform convergence analysis of both methods and conduct a number of experiments highlighting the benefits of our approach. <br>
                <a href="https://cemse.kaust.edu.sa/amcs/people/person/grigory-malinovsky">Grigory Malinovsky</a>, <a href="https://dmitry-kovalev.com">Dmitry Kovalev</a>, Elnur Gasanov, <a href="https://lcondat.github.io">Laurent Condat</a>,  <a href="https://richtarik.org/index.html">Peter Richt&aacuterik</a><br>
                <a href="http://proceedings.mlr.press/v119/malinovskiy20a/malinovskiy20a.pdf">ICML 2020</a>
                </div><br>

                <div>
                <a href="https://arxiv.org/abs/1802.03703"> <papertitle>Stochastic Spectral and Conjugate Descent Methods</papertitle></a><br>
                The state-of-the-art methods for solving optimization problems in big dimensions are variants of randomized coordinate descent (RCD). In this paper we introduce a fundamentally new type of acceleration strategy for RCD based on the augmenta- tion of the set of coordinate directions by a few spectral or conjugate directions. As we increase the number of extra directions to be sampled from, the rate of the method improves, and interpolates between the linear rate of RCD and a linear rate independent of the condition number. We develop and analyze also inexact variants of these methods where the spectral and conjugate directions are allowed to be approximate only. We motivate the above development by proving several negative results which highlight the limitations of RCD with importance sampling. <br>
                <a href="https://dmitry-kovalev.com">Dmitry Kovalev</a>, <a href="https://eduardgorbunov.github.io">Eduard Gorbunov</a>, Elnur Gasanov, <a href="https://richtarik.org/index.html">Peter Richt&aacuterik</a><br>
                <a href="https://papers.nips.cc/paper/7596-stochastic-spectral-and-conjugate-descent-methods.pdf">NeurIPS 2018</a>
                </div><br>

                <div>
                <a href="http://jmlda.org/papers/doc/2017/no2/Gasanov2017ECoGAnalysis.pdf"> <papertitle> Creation of approximating scalogram description in a problem of movement prediction [in Russian]</papertitle></a><br>
                The paper addresses the problem of a thumb movement prediction using electrocorticographic (ECoG) activity. The task is to predict thumb positions from the voltage time series of cortical activity. The scalograms are used as input features to this regression problem. Scalograms are generated by the spatio-spectro-temporal integration of voltage time series across multiple cortical areas. To reduce the dimension of a feature space, local approximation is used: every scalogram is approximated by parametric model. The predictions are obtained with partial least squares regression applied to local approximation parameters. Local approximation of scalograms does not significantly lower the quality of prediction while it efficiently reduces the dimension of feature space.<br>
                Elnur Gasanov, <a href="https://www.researchgate.net/profile/Anastasia_Motrenko2">Motrenko Anastasia</a><br>
                <a href="http://jmlda.org/papers/doc/2017/no2/Gasanov2017ECoGAnalysis.pdf">Journal of Machine Learning and Data Analysis (in Russian)</a>
                </div><br>
            </td>
         </tr>
        </tbody>
</table>

<!--
     <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <heading style="color: blue;">Publications</heading>
                <p>
                <ul>
                    <li> Example</li>
                </ul>
                </p>
            </td>
         </tr>
        </tbody>
</table>
-->


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info">This guy makes a cool webpage</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
